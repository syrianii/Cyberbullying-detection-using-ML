{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclass Cyberbullying dectection Notebook"
      ],
      "metadata": {
        "id": "YlgN_QtHQIiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### installing necessary libraries"
      ],
      "metadata": {
        "id": "R2KJwTPjCMch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2TLl1DMXKJg",
        "outputId": "176d799e-0897-4026-b5a6-9eb9148eb189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.5.1.tar.gz (356 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m356.3/356.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.5.1-py2.py3-none-any.whl size=351210 sha256=5da415e1aefe5866ebd736b1382db408756e618b193097acb9af8b4ef26944a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/92/44/e2ef13f803aa08711819357e6de0c5fe67b874671141413565\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=dacfc4aa1783b6061061965d4ff0039c4caaa338092999cc4f89a463f00004d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.4.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.3)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow)\n",
            "  Downloading databricks-cli-0.17.7.tar.gz (83 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Collecting gitpython<4,>=2.1.0 (from mlflow)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2022.7.1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.27.1)\n",
            "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.1)\n",
            "Collecting importlib-metadata!=4.7.0,<7,>=3.7.0 (from mlflow)\n",
            "  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<7,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.22.4)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.10.1)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.5.3)\n",
            "Collecting querystring-parser<2 (from mlflow)\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.16)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: pyarrow<13,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Collecting gunicorn<21 (from mlflow)\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.2)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.6.3)\n",
            "Collecting pyjwt>=1.7.0 (from databricks-cli<1,>=0.8.7->mlflow)\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.10)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.16)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow) (1.6.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.3.6)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.1.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.10/dist-packages (from gunicorn<21->mlflow) (67.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: databricks-cli\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.7-py3-none-any.whl size=143860 sha256=268b0df397408f373550b1e166f40e66d30ffed65c0c6eb9096413a5c8c1ec88\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/63/93/5402c1a09c1868a59d0b05013484e07af97a9d7b3dbd5bd39a\n",
            "Successfully built databricks-cli\n",
            "Installing collected packages: smmap, querystring-parser, pyjwt, Mako, importlib-metadata, gunicorn, gitdb, docker, databricks-cli, alembic, gitpython, mlflow\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 databricks-cli-0.17.7 docker-6.1.3 gitdb-4.0.10 gitpython-3.1.31 gunicorn-20.1.0 importlib-metadata-6.7.0 mlflow-2.4.1 pyjwt-2.7.0 querystring-parser-1.2.4 smmap-5.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=773907721166352afdebac9eeedc64ea1a49ecad52b10fd3f060a5e552035104\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "!pip install contractions\n",
        "!pip install langdetect\n",
        "!pip install mlflow\n",
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries"
      ],
      "metadata": {
        "id": "4lHuYbMiClQC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixu2bLcQtwnn",
        "outputId": "6e0ba6cd-69c8-48fd-f8ea-f8a5d06717ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "import unicodedata\n",
        "import contractions\n",
        "from langdetect import detect\n",
        "import nltk\n",
        "import seaborn as sns\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import learning_curve,ShuffleSplit\n",
        "from sklearn.metrics import roc_auc_score,confusion_matrix,roc_curve,auc,mean_squared_error,mean_absolute_error,r2_score,f1_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLflow"
      ],
      "metadata": {
        "id": "q0Fts2KlC0Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment('ML-project1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMpQcO7Beq9b",
        "outputId": "6a23086b-021e-4e79-fda2-04a14bbfb497"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/06/26 08:41:25 INFO mlflow.tracking.fluent: Experiment with name 'ML-project1' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///content/mlruns/779627010302941359', creation_time=1687768885710, experiment_id='779627010302941359', last_update_time=1687768885710, lifecycle_stage='active', name='ML-project1', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA"
      ],
      "metadata": {
        "id": "OmADcucAC8ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### importing the dataset"
      ],
      "metadata": {
        "id": "6VxNg-hDDBMc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "RdUcWRZhuNrB",
        "outputId": "aa4dc323-5a32-43ab-b306-f8dfd23ed46b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6aa058840fb1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '***'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"***\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kivje3mv3gvz"
      },
      "outputs": [],
      "source": [
        "original_df = df.copy()\n",
        "original_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### removing the duplicated data"
      ],
      "metadata": {
        "id": "01QT6rsdDHCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msc8IwNzJX5a"
      },
      "outputs": [],
      "source": [
        "print('length before dropping duplicates',len(df))\n",
        "df = df.drop_duplicates()\n",
        "df = df.replace('',np.nan)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print('Length after dropping duplicates: ',len(df))\n",
        "print('Length of the original dataframe:',len(original_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking for imbalance"
      ],
      "metadata": {
        "id": "YMRmE2e5DKs2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHflAM0CJbIB"
      },
      "outputs": [],
      "source": [
        "df['cyberbullying_type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the Data"
      ],
      "metadata": {
        "id": "eLyXwtk5DNyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvaZZoXcJthZ"
      },
      "outputs": [],
      "source": [
        "df.groupby('cyberbullying_type').size().plot(kind='pie',autopct='%1.1f%%');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Preprocessing"
      ],
      "metadata": {
        "id": "JFGNkw_JDR01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text cleaning functions"
      ],
      "metadata": {
        "id": "k3n42NUoIJnw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYZ5N6_6KE5G"
      },
      "outputs": [],
      "source": [
        "def emoji_to_text(text):\n",
        "    if type(text) != float:\n",
        "        return emoji.demojize(text)\n",
        "    else:\n",
        "        return text\n",
        "print(emoji_to_text('RT @mykitchenrules: Nawwww üò≠üò≠üò≠ #MKR'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7GsNybmAJLT"
      },
      "outputs": [],
      "source": [
        "def remove_underscore_dash(text):\n",
        "  return re.sub(r\"(-|_)\",\" \",text)\n",
        "\n",
        "print(remove_underscore_dash(\"RT @mykitchenrules: Nawwww :loudly_crying_face::loudly_crying_face::loudly_crying_face: #MKR\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3a6KWVfKzEM"
      },
      "outputs": [],
      "source": [
        "def remove_hashtags(text):\n",
        "    return text.replace('#','')\n",
        "print(remove_hashtags('#simpleasthat'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajdeiYUvhu7k"
      },
      "outputs": [],
      "source": [
        "def remove_mention(text):\n",
        "  return re.sub(r\"@[^\\s]+\",\"\",text)\n",
        "\n",
        "print(remove_mention('@girlziplocked jfc.'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fq48ZNgi2wW"
      },
      "outputs": [],
      "source": [
        "def remove_link(text):\n",
        "  return re.sub(r\"http[^s]+\",\"\",text)\n",
        "print(remove_link('@Vrais66 And these. http://t.co/V3cf0aGlcp'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9X7ThTekLh2"
      },
      "outputs": [],
      "source": [
        "def remove_repeated_chars(text):\n",
        "  return re.sub(r\"(.)\\1\\1+\",r\"\\1\",text)\n",
        "print(remove_repeated_chars('‚Äú@PublimetroMX: Ninel Conde, v√≠ctima del bullying en Twitter, cambia su cuenta http://t.co/pFE73I7‚Äù/// aaaaaaaaaajajajajajajajahahahajahaja'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tip2NXPWk3cu"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(text):\n",
        "  return re.sub(r'[0-9]','',text)\n",
        "\n",
        "print(remove_numbers('@scholarshipscom My 2013 education resolution is to go to school for pharmacy and be successful.'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57iOFzv3nrbc"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "  if not (re.match(r'#(\\w+)', text)):\n",
        "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "  return text\n",
        "\n",
        "\n",
        "print(remove_punctuation('My fav #MKR teams so far are definitely: Eva &amp; Debra Rose &amp; Josh Annie &amp; Loyd Robert &amp; Lynzy Sherie &amp; Emilie Jaz &amp; Shaz (not in order)'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX3RrOL0ooJw"
      },
      "outputs": [],
      "source": [
        "def remove_non_english_char(text):\n",
        "  return  \"\".join(([idx for idx in text if not re.findall(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\", idx)]))\n",
        "\n",
        "print(remove_non_english_char(\"kdls # ! 0L ŸÜŸäŸÜÿµŸá\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2241Z06s3pY"
      },
      "outputs": [],
      "source": [
        "def remove_whitespace(text):\n",
        "\n",
        "    text = re.sub(r'^\\s+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip('\\'')\n",
        "\n",
        "    return text\n",
        "\n",
        "print(remove_whitespace('@dsmyxe @PolitiBunny the end        result is.         the same.'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PDIBepiwEP3"
      },
      "outputs": [],
      "source": [
        "def lower_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "print(lower_text('Dont like liver #mkr #mkr2015'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V27Jv5kvxilQ"
      },
      "outputs": [],
      "source": [
        "def remove_accents(text):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
        "    text = nfkd_form.encode('ASCII', 'ignore').decode('utf-8')\n",
        "    return text\n",
        "print(remove_accents('Read my response to \"J√° sofreram bullying?\": http://t.co/sbEIykB'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVrZvpbx604B"
      },
      "outputs": [],
      "source": [
        "def remove_contractions(text):\n",
        "  expanded_words = []\n",
        "  for word in text.split():\n",
        "      expanded_words.append(contractions.fix(word))\n",
        "\n",
        "  return ' '.join(expanded_words)\n",
        "\n",
        "print(remove_contractions(\"isn't chilling\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxEktDHGaDVr"
      },
      "outputs": [],
      "source": [
        "def remove_non_english_text(text):\n",
        "  print(detect(text))\n",
        "\n",
        "remove_non_english_text(\"Frio √© bullying contra os solteiros..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRLEEjQoEf-p"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_duplicated_emojies(text):\n",
        "  return re.sub(r\"([\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\\U00002600-\\U000027BF])\\1+\",r'\\1',text)\n",
        "\n",
        "remove_duplicated_emojies(\"Twitter makes me laugh....Better than the series ;)))))) Very funny üëèüëèüëèüëèüëè  #MKR\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9R-VpM9DXhi"
      },
      "outputs": [],
      "source": [
        "def remove_duplicated_words(text):\n",
        "  return re.sub(r\"\\\\b(\\\\w+)(?:\\\\W+\\\\1\\\\b)+\",'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUhgOJ84kn_-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_stopwords(text,flag):\n",
        "  if flag:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "  else:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.update(['RT','as','go','rt','would','know','one','u','amp','mkr'],)\n",
        "  tokenizer = TweetTokenizer()\n",
        "  tokenized = tokenizer.tokenize(text)\n",
        "  removed_sentence = [word for word in tokenized if not word.lower() in stop_words]\n",
        "  return ' '.join(removed_sentence)\n",
        "\n",
        "remove_stopwords('in other words mkr rt RT katandandre your food was crapi..',flag = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxEcXfCllDem"
      },
      "outputs": [],
      "source": [
        "def stemmer(text):\n",
        "   tokenizer = TweetTokenizer()\n",
        "   tokenized = tokenizer.tokenize(text)\n",
        "   stemmer = PorterStemmer()\n",
        "   s = []\n",
        "   for token in tokenized:\n",
        "      token = stemmer.stem(token)\n",
        "      s.append(token)\n",
        "   return ' '.join(s)\n",
        "\n",
        "stemmer('words katandandre food crapilicious mkr')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu_7JdwPU7mA"
      },
      "outputs": [],
      "source": [
        "def lemmatizer(text):\n",
        "  tokenizer = TweetTokenizer()\n",
        "  tokenized = tokenizer.tokenize(text)\n",
        "  lemmatizerr = WordNetLemmatizer()\n",
        "  s = []\n",
        "  for token in tokenized:\n",
        "    token = lemmatizerr.lemmatize(token)\n",
        "    s.append(token)\n",
        "  return ' '.join(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61NsttkzAJnR"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text,flag):\n",
        "\n",
        "  \"\"\"\n",
        "  Process the text and return it to a clean one.\n",
        "  Input:\n",
        "        text: a string containing a text.\n",
        "  Output:\n",
        "        cleaned_text : text after applying all the cleaning functions.\n",
        "  \"\"\"\n",
        "  text = remove_mention(text)\n",
        "  text = remove_link(text)\n",
        "  text = remove_hashtags(text)\n",
        "  text = remove_duplicated_emojies(text)\n",
        "  text = emoji_to_text(text)\n",
        "  text = remove_repeated_chars(text)\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_underscore_dash(text)\n",
        "  text = remove_contractions(text)\n",
        "  text = remove_non_english_char(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = remove_accents(text)\n",
        "  text = lower_text(text)\n",
        "  text = remove_duplicated_words(text)\n",
        "  text = remove_whitespace(text)\n",
        "  text = remove_stopwords(text,flag)\n",
        "  text = lemmatizer(text)\n",
        "\n",
        "\n",
        "  return text\n",
        "\n",
        "preprocess_text(\"Twitter make me laugh....Better than the series ;)))))) Very funny üëèüëèüëèüëèüëè  #MKR\",False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### applying the above functions on the data"
      ],
      "metadata": {
        "id": "08BVcl-IIPOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_xxkZqBZ3-m"
      },
      "outputs": [],
      "source": [
        "df['processed_text'] = df['tweet_text'].apply(lambda text: preprocess_text(text,flag=True))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### checking again for Nan/empty/duplicated values & imbalaced classes"
      ],
      "metadata": {
        "id": "SRGf-XK0Dpi0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPJxxIfwoem_"
      },
      "outputs": [],
      "source": [
        "df['processed_text'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCWZ5y-EonHA"
      },
      "outputs": [],
      "source": [
        "if df['processed_text'].str.strip().empty:\n",
        "  print('there are empty values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i4xsKseMUAx"
      },
      "outputs": [],
      "source": [
        "df[\"processed_text\"].duplicated().sum()\n",
        "df.drop_duplicates(\"processed_text\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEtxQ-0j_PAd"
      },
      "outputs": [],
      "source": [
        "df.cyberbullying_type.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing the other_cyberbullying type class"
      ],
      "metadata": {
        "id": "1-uRlRvyI5vF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrR3H6zRNDVV"
      },
      "outputs": [],
      "source": [
        "df.drop(df[df['cyberbullying_type'] == 'other_cyberbullying'].index, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking again for class balance"
      ],
      "metadata": {
        "id": "jcH-S6JCI_BN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S96BajqDNSEl"
      },
      "outputs": [],
      "source": [
        "df.cyberbullying_type.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cleaned Data visualization"
      ],
      "metadata": {
        "id": "T4cHpyCZD2CU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXx2vcwN5y-S"
      },
      "outputs": [],
      "source": [
        "class_counts = df['cyberbullying_type'].value_counts()\n",
        "class_counts.plot(kind='bar')\n",
        "plt.title('Class Distribution of Cyberbullying Types')\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NEfuW9zNT-z"
      },
      "outputs": [],
      "source": [
        "text_len = []\n",
        "for text in df.processed_text:\n",
        "    tweet_len = len(text.split())\n",
        "    text_len.append(tweet_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kjW_vdDNx77"
      },
      "outputs": [],
      "source": [
        "df['text_len'] = text_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1dO0R6RN31i"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n",
        "plt.title('Count of tweets with less than 10 words', fontsize=20)\n",
        "plt.yticks([])\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzLw1NfHN7vx"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by=['text_len'], ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl66nFCwOMD4"
      },
      "source": [
        "#### Removing ouliers\n",
        " removing words less that 4 word and more than 100 as they can be outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ePDtZpROjcD"
      },
      "outputs": [],
      "source": [
        "df = df[df['text_len'] > 3]\n",
        "df = df[df['text_len'] < 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJQIPQoaOkGy"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### data visualization using word cloud"
      ],
      "metadata": {
        "id": "y-HSckGWE9wH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40T3FlvZ6yJG"
      },
      "outputs": [],
      "source": [
        "print(\"Gender\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='gender'].tweet_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Ethnicity\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='ethnicity'].tweet_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Religion\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='religion'].tweet_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Age\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='age'].tweet_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Not Cyberbullying\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='not_cyberbullying'].tweet_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**checking the top 50 unique words as some of them could be outliers and we can remove them from out data**"
      ],
      "metadata": {
        "id": "4nYQZzD0JZxx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zphyJ4GkQV_E"
      },
      "outputs": [],
      "source": [
        "for bully in df['cyberbullying_type'].unique():\n",
        "    top_50_words=df.processed_text[df['cyberbullying_type']==bully].str.split(expand=True).stack().value_counts()[:51]\n",
        "top_50_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing the words that are outlieres by adding them to the stopwords list**"
      ],
      "metadata": {
        "id": "qdvoWxJGJnMI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5Ja2dK66-42"
      },
      "outputs": [],
      "source": [
        "df['processed_text'] = df['tweet_text'].apply(lambda text: preprocess_text(text,flag=False))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### visualization using word cloud after removing outliers"
      ],
      "metadata": {
        "id": "btUkNXvZFILM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h856dDaqTamv"
      },
      "outputs": [],
      "source": [
        "print(\"Gender\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='gender'].processed_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Ethnicity\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='ethnicity'].processed_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Religion\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='religion'].processed_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Age\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='age'].processed_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Not Cyberbullying\")\n",
        "text = \" \".join(review for review in df[df.cyberbullying_type=='not_cyberbullying'].processed_text.astype(str))\n",
        "wordcloud = WordCloud().generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding"
      ],
      "metadata": {
        "id": "oTB3zjBNFTmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erFmwnl4OlJx"
      },
      "outputs": [],
      "source": [
        "#### encoding cyberbullying types\n",
        "encoder=LabelEncoder()\n",
        "df['cyberbullying_type_encoded']=encoder.fit_transform(df['cyberbullying_type'])\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "9S50zWTsFZoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding new columns by making some calculations on the tweets to check we can have relations between them**"
      ],
      "metadata": {
        "id": "NDWLNbBvKakF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0W2_6NVWn-R"
      },
      "outputs": [],
      "source": [
        "def count_chars(text):\n",
        "  return len(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI2KljOoZDTu"
      },
      "outputs": [],
      "source": [
        "def count_words(text):\n",
        "  return len(text.split())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlAgIpd5ZOMM"
      },
      "outputs": [],
      "source": [
        "def count_capital_chars(text):\n",
        "  count = 0\n",
        "  for i in text:\n",
        "    if i.isupper():\n",
        "      count+=1\n",
        "  return count\n",
        "\n",
        "count_capital_chars(\"Hello MY Dear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Daa2_F4rZpzb"
      },
      "outputs": [],
      "source": [
        "def count_capital_words(text):\n",
        "  return sum(map(str.isupper,text.split()))\n",
        "\n",
        "count_capital_words(\"Hello MY Dear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMPlJ1t9ainf"
      },
      "outputs": [],
      "source": [
        "def count_sent(text):\n",
        "    return len(nltk.sent_tokenize(text))\n",
        "\n",
        "count_sent('hello, rami. hello.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65fqHblfbjFW"
      },
      "outputs": [],
      "source": [
        "def count_unique_words(text):\n",
        "  unique_words = set(word.lower() for word in text.split())\n",
        "  return len(unique_words)\n",
        "count_unique_words('hello rami Hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sdkfgUtbz4j"
      },
      "outputs": [],
      "source": [
        "def count_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenizer = TweetTokenizer()\n",
        "  tokenized = tokenizer.tokenize(text)\n",
        "  stopwords_ = [w for w in tokenized if w in stop_words]\n",
        "  return len(stopwords_)\n",
        "\n",
        "count_stopwords(\"Hello is are i am this that\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating the new columns"
      ],
      "metadata": {
        "id": "26Q0jnMtKoIl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQorvabfc2_4"
      },
      "outputs": [],
      "source": [
        "df['char_count'] = df['tweet_text'].apply(lambda text: count_chars(text))\n",
        "df['word_count'] = df['tweet_text'].apply(lambda text: count_words(text))\n",
        "df['sent_count'] = df['tweet_text'].apply(lambda text: count_sent(text))\n",
        "df['capital_char_count']= df['tweet_text'].apply(lambda text: count_capital_chars(text))\n",
        "df['capital_word_count'] = df['tweet_text'].apply(lambda text: count_capital_words(text))\n",
        "df['stopword_count'] = df['tweet_text'].apply(lambda text: count_stopwords(text))\n",
        "df['unique_word_count'] = df['tweet_text'].apply(lambda text: count_unique_words(text))\n",
        "df['avg_wordlength'] = df['char_count'] / df ['word_count']\n",
        "df['avg_senlength'] = df['word_count'] / df['sent_count']\n",
        "df['unique_vs_words'] = df['unique_word_count'] / df['word_count']\n",
        "df['stopwords_vs_words'] = df['stopword_count'] / df['word_count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcvWhEZ-r__t"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdGS4wums4aZ"
      },
      "source": [
        "### Feature extraction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfc5xRaNY4zc"
      },
      "source": [
        "#### tf-idf & BOW vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgLdIwVr6leP"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfTransformer()\n",
        "clf = CountVectorizer()\n",
        "\n",
        "X1_cv = clf.fit_transform(df['processed_text'])\n",
        "\n",
        "tf1_transformer = TfidfTransformer(use_idf=True).fit(X1_cv)\n",
        "X1_tf = tf1_transformer.transform(X1_cv)\n",
        "vocabulary = clf.vocabulary_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSQXQ-P38JAD"
      },
      "outputs": [],
      "source": [
        "X1_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKv25_5vXEPT"
      },
      "outputs": [],
      "source": [
        "y = df['cyberbullying_type_encoded']\n",
        "X1_train, X1_test, y_train, y_test = train_test_split(X1_tf, y,test_size= 0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for class imbalance and as we can see that the clases after splitted are imbalanced**"
      ],
      "metadata": {
        "id": "aUIijXlcLbQp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26h7qGELaQt6"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaMfxX9wOXJe"
      },
      "source": [
        "### Balancing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRdw_0yQaTda"
      },
      "outputs": [],
      "source": [
        "vc = y_train.value_counts()\n",
        "while (vc[0] != vc[4]) or (vc[0] != vc[2]) or (vc[0] != vc[3]) or (vc[0] != vc[1]):\n",
        "  smote = SMOTE(sampling_strategy='minority')\n",
        "  X1_train, y_train = smote.fit_resample(X1_train,y_train)\n",
        "  vc = y_train.value_counts()\n",
        "\n",
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcef5BBsLRcn"
      },
      "outputs": [],
      "source": [
        "sentiments = [\"religion\",\"age\",\"gender\",\"ethnicity\",\"not bullying\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Algorithms"
      ],
      "metadata": {
        "id": "DoVSe7RzMe26"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLgzK2O94g9p"
      },
      "source": [
        "#### Naive bais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dtNrS9-YI2d"
      },
      "outputs": [],
      "source": [
        "def Naive_bias(X_train, y_train, X_test, y_test):\n",
        "    with mlflow.start_run():\n",
        "        nb_clf = MultinomialNB()\n",
        "        nb_clf.fit(X_train, y_train)\n",
        "        nb_pred = nb_clf.predict(X_test)\n",
        "        nb_pred_train = nb_clf.predict(X_train)\n",
        "\n",
        "        f1_score_test = f1_score(y_test, nb_pred, average='weighted')\n",
        "        f1_score_train = f1_score(y_train, nb_pred_train, average='weighted')\n",
        "\n",
        "        mlflow.log_metric(\"f1_score_test\", f1_score_test)\n",
        "        mlflow.log_metric(\"f1_score_train\", f1_score_train)\n",
        "        mlflow.sklearn.log_model(nb_clf, \"naive bayes classifier\")\n",
        "\n",
        "        print('accuracy score (train) :', accuracy_score(y_train, nb_pred_train))\n",
        "        print('accuracy score  (test) :', accuracy_score(y_test, nb_pred))\n",
        "        print('F1 score (train) :', f1_score_train)\n",
        "        print('F1 score (test) :', f1_score_test)\n",
        "\n",
        "        return classification_report(y_test, nb_pred, target_names=sentiments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrRBFfu46Lws"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7bvqqEZ6PiM"
      },
      "outputs": [],
      "source": [
        "def Random_forest(X_train, y_train, X_test,y_test):\n",
        "   with mlflow.start_run():\n",
        "        rf_clf = RandomForestClassifier()\n",
        "        rf_clf.fit(X_train, y_train)\n",
        "        rf_pred = rf_clf.predict(X_test)\n",
        "        rf_pred_train = rf_clf.predict(X_train)\n",
        "\n",
        "        f1_score_test = f1_score(y_test, rf_pred, average='weighted')\n",
        "        f1_score_train = f1_score(y_train, rf_pred_train, average='weighted')\n",
        "\n",
        "        mlflow.log_metric(\"f1_score_test\", f1_score_test)\n",
        "        mlflow.log_metric(\"f1_score_train\", f1_score_train)\n",
        "        mlflow.sklearn.log_model(rf_clf, \"Random Forest classifier\")\n",
        "\n",
        "        print('accuracy score (train) :', accuracy_score(y_train, rf_pred_train))\n",
        "        print('accuracy score (test) :', accuracy_score(y_test, rf_pred))\n",
        "        print('F1 score (train) :', f1_score_train)\n",
        "        print('F1 score (test) :', f1_score_test)\n",
        "\n",
        "        return classification_report(y_test, rf_pred, target_names=sentiments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aY4aiIhZ33Y"
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkpR-SEDYnLD"
      },
      "outputs": [],
      "source": [
        "def Decision_tree(X_train, y_train, X_test,y_test):\n",
        "  with mlflow.start_run():\n",
        "        dt_clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "        dt_clf.fit(X_train, y_train)\n",
        "        dt_pred = dt_clf.predict(X_test)\n",
        "        dt_pred_train = dt_clf.predict(X_train)\n",
        "\n",
        "        f1_score_test = f1_score(y_test, dt_pred, average='weighted')\n",
        "        f1_score_train = f1_score(y_train, dt_pred_train, average='weighted')\n",
        "\n",
        "        mlflow.log_metric(\"f1_score_test\", f1_score_test)\n",
        "        mlflow.log_metric(\"f1_score_train\", f1_score_train)\n",
        "        mlflow.sklearn.log_model(dt_clf, \"decision tree classifier\")\n",
        "\n",
        "        print('accuracy score (train) :', accuracy_score(y_train, dt_pred_train))\n",
        "        print('accuracy score (test) :', accuracy_score(y_test, dt_pred))\n",
        "        print('F1 score (train) :', f1_score_train)\n",
        "        print('F1 score (test) :', f1_score_test)\n",
        "\n",
        "        return classification_report(y_test, dt_pred, target_names=sentiments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7sbsQyjOg9J"
      },
      "source": [
        "#### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFN3loj_aGFK"
      },
      "outputs": [],
      "source": [
        "def Svc(X_train, y_train, X_test, y_test):\n",
        "  with mlflow.start_run():\n",
        "        svc_clf = svm.SVC(kernel='linear', C=1, gamma=0)\n",
        "        svc_clf.fit(X_train, y_train)\n",
        "        svc_pred = svc_clf.predict(X_test)\n",
        "        svc_pred_train = svc_clf.predict(X_train)\n",
        "\n",
        "        f1_score_test = f1_score(y_test, svc_pred, average='weighted')\n",
        "        f1_score_train = f1_score(y_train, svc_pred_train, average='weighted')\n",
        "\n",
        "        mlflow.log_metric(\"f1_score_test\", f1_score_test)\n",
        "        mlflow.log_metric(\"f1_score_train\", f1_score_train)\n",
        "        mlflow.sklearn.log_model(svc_clf, \"svm classifier\")\n",
        "\n",
        "        print('accuracy score (train) :', accuracy_score(y_train, svc_pred_train))\n",
        "        print('accuracy score (test) :', accuracy_score(y_test, svc_pred))\n",
        "        print('F1 score (train) :', f1_score_train)\n",
        "        print('F1 score (test) :', f1_score_test)\n",
        "\n",
        "        return classification_report(y_test, svc_pred, target_names=sentiments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run algorithm"
      ],
      "metadata": {
        "id": "p7G8cpOsNEKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTdaqkKY9AK8"
      },
      "outputs": [],
      "source": [
        "def run_algorithm(X_train, y_train, X_test,y_test):\n",
        "  print('Naive Bayes:')\n",
        "  print(\"Classification Report for Naive Bias :\",Naive_bias(X_train,y_train,X_test,y_test))\n",
        "  print('______________________________________________________________________')\n",
        "  print('Random Forest :')\n",
        "  print(\"Classification Report for Random Forest :\",Random_forest(X_train,y_train,X_test,y_test))\n",
        "  print('______________________________________________________________________')\n",
        "  print('Decision Tree :')\n",
        "  print(\"Classification Reportj for Decision Tree:\",Decision_tree(X_train, y_train, X_test, y_test))\n",
        "  print('______________________________________________________________________')\n",
        "  print('Support Vector Machines: ')\n",
        "  print('Classification Report for Support Vector Machines',Svc(X_train, y_train, X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcQqh47DNMcV"
      },
      "outputs": [],
      "source": [
        "run_algorithm(X1_train, y_train, X1_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr_TWCQ8RBy_"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ8KLmEE273z"
      },
      "source": [
        "#### Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIj5hd6M3Atk"
      },
      "outputs": [],
      "source": [
        "def svc_gridSCV(X_train,y_train,X_test,y_test):\n",
        "  param_grid = {\n",
        "    'C': uniform(loc=0, scale=10),\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
        "    'degree': [1,2, 3,] }\n",
        "  svc_clf = svm.SVC()\n",
        "  random_search = RandomizedSearchCV(svc_clf,param_distributions = param_grid, n_iter=10 , cv=5,scoring='accuracy',random_state=42)\n",
        "  random_search.fit(X_train,y_train)\n",
        "  svc_rs_pred = random_search.predict(X_test)\n",
        "  svc_rs_pred_train = random_search.predict(X_train)\n",
        "  print('accuracy score (train) :',accuracy_score(y_train,svc_rs_pred_train))\n",
        "  print('accuracy score (test) :', accuracy_score(y_test, svc_rs_pred))\n",
        "\n",
        "  print(\"Best Parameters: \", random_search.best_params_)\n",
        "  print(\"Best Score: \", random_search.best_score_)\n",
        "\n",
        "  return classification_report(y_test,svc_rs_pred,target_names = sentiments)\n",
        "\n",
        "\n",
        "#svc_gridSCV(X1_train, y_train, X1_test,y_test)\n",
        "\n",
        "\n",
        "# accuracy score (train) : 0.9996851881001102\n",
        "# accuracy score (test) : 0.9316135966680102\n",
        "# Best Parameters:  {'C': 5.247564316322379, 'degree': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
        "# Best Score:  0.9389894538013538\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     religion       0.96      0.97      0.96      1537\n",
        "#          age       0.98      0.98      0.98      1571\n",
        "#       gender       0.95      0.87      0.91      1462\n",
        "#    ethnicity       0.79      0.87      0.83      1320\n",
        "# not bullying       0.96      0.95      0.96      1553\n",
        "\n",
        "#     accuracy                           0.93      7443\n",
        "#    macro avg       0.93      0.93      0.93      7443\n",
        "# weighted avg       0.93      0.93      0.93      7443\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRqW6Ppl53JF"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZnTDJBqKdHo"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rf_gridSCV(X_train,y_train,X_test,y_test):\n",
        "\n",
        "  param_grid = {\n",
        "    'n_estimators': [10, 50, 100, 200, 500],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None] }\n",
        "\n",
        "  rfŸÄclf = RandomForestClassifier()\n",
        "  random_search = RandomizedSearchCV(rfŸÄclf,param_distributions = param_grid, n_iter=10 , cv=5,random_state=42)\n",
        "  random_search.fit(X_train,y_train)\n",
        "  rf_rs_pred = random_search.predict(X_test)\n",
        "  rf_rs_pred_train = random_search.predict(X_train)\n",
        "  print('accuracy score (train) :',accuracy_score(y_train,rf_rs_pred_train))\n",
        "  print('accuracy score (test) :', accuracy_score(y_test, rf_rs_pred))\n",
        "\n",
        "  print(\"Best Parameters: \", random_search.best_params_)\n",
        "  print(\"Best Score: \", random_search.best_score_)\n",
        "\n",
        "  return classification_report(y_test,rf_rs_pred,target_names = sentiments)\n",
        "\n",
        "#rf_gridSCV(X1_train, y_train, X1_test,y_test)\n",
        "\n",
        "\n",
        "# accuracy score (train) : 0.9272469699354635\n",
        "# accuracy score (test) : 0.9181781539701733\n",
        "# Best Parameters:  {'n_estimators': 50, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 20}\n",
        "# Best Score:  0.9173933574689123\n",
        "\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     religion       0.99      0.96      0.98      1537\n",
        "#          age       0.98      0.96      0.97      1571\n",
        "#       gender       0.98      0.80      0.88      1462\n",
        "#    ethnicity       0.71      0.95      0.81      1320\n",
        "# not bullying       0.97      0.91      0.94      1553\n",
        "\n",
        "#     accuracy                           0.92      7443\n",
        "#    macro avg       0.93      0.92      0.92      7443\n",
        "# weighted avg       0.93      0.92      0.92      7443\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWYluMur6CX6"
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luz-xgJH4lKL"
      },
      "outputs": [],
      "source": [
        "def dt_gridSCV(X_train,y_train,X_test,y_test):\n",
        "\n",
        "  param_grid = {\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "  }\n",
        "  dt_clf = DecisionTreeClassifier()\n",
        "  random_search = RandomizedSearchCV(dt_clf,param_distributions = param_grid, n_iter=10 , cv=5,random_state=42)\n",
        "  random_search.fit(X_train,y_train)\n",
        "  dt_rs_pred = random_search.predict(X_test)\n",
        "  dt_rs_pred_train = random_search.predict(X_train)\n",
        "  print('accuracy score (train) :',accuracy_score(y_train,dt_rs_pred_train))\n",
        "  print('accuracy score (test) :', accuracy_score(y_test, dt_rs_pred))\n",
        "\n",
        "  print(\"Best Parameters: \", random_search.best_params_)\n",
        "  print(\"Best Score: \", random_search.best_score_)\n",
        "\n",
        "  return classification_report(y_test,dt_rs_pred,target_names = sentiments)\n",
        "\n",
        "\n",
        "#dt_gridSCV(X1_train, y_train, X1_test,y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# accuracy score (train) : 0.85997166692901\n",
        "# accuracy score (test) : 0.847776434233508\n",
        "# Best Parameters:  {'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 10}\n",
        "# Best Score:  0.854525421060916\n",
        "\n",
        "\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     religion       0.99      0.93      0.96      1537\n",
        "#          age       0.98      0.86      0.92      1571\n",
        "#       gender       0.99      0.64      0.77      1462\n",
        "#    ethnicity       0.55      0.97      0.70      1320\n",
        "# not bullying       0.97      0.85      0.90      1553\n",
        "\n",
        "#     accuracy                           0.85      7443\n",
        "#    macro avg       0.90      0.85      0.85      7443\n",
        "# weighted avg       0.91      0.85      0.86      7443\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ensemble learning"
      ],
      "metadata": {
        "id": "TYme2TbKO-zE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CprJa63krwiP"
      },
      "source": [
        "#### Soft Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59lBB9qrzHPV"
      },
      "outputs": [],
      "source": [
        "def soft_voting(X_train, y_train, X_test, y_test):\n",
        "  with mlflow.start_run():\n",
        "    svm_clf = svm.SVC(C= 5.247564316322379, degree= 1, gamma= 'scale', kernel= 'rbf', probability=True)\n",
        "    rf_clf = RandomForestClassifier(n_estimators= 50, min_samples_split= 10, min_samples_leaf= 2, max_features= None, max_depth= 20, bootstrap=True , oob_score=True)\n",
        "    dt_clf = DecisionTreeClassifier(min_samples_split= 10, min_samples_leaf= 2, max_features= None, max_depth= 10)\n",
        "\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=[('rf', rf_clf), ('dt', dt_clf)],\n",
        "        voting='soft',\n",
        "        flatten_transform=True,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    title = \"Learning Curves (Soft Voting Classifier)\"\n",
        "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
        "    plot_learning_curve(voting_clf, title, X_train, y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=-1)\n",
        "\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "\n",
        "    voting_clf_pred = voting_clf.predict(X_test)\n",
        "    voting_clf_train = voting_clf.predict(X_train)\n",
        "    rmse,mae,r2 = eval(y_test,voting_clf_pred)\n",
        "    f1_score_test = f1_score(y_test, rf_pred, average='weighted')\n",
        "    f1_score_train = f1_score(y_train, rf_pred_train, average='weighted')\n",
        "\n",
        "    mlflow.log_metric(\"f1_score_test\", f1_score_test)\n",
        "    mlflow.log_metric(\"f1_score_train\", f1_score_train)\n",
        "    mlflow.sklearn.log_model(voting_clf,\"softvoting classifier\")\n",
        "\n",
        "\n",
        "\n",
        "    print('accuracy score (train) :',accuracy_score(y_train,voting_clf_train))\n",
        "    print('accuracy score (test) :', accuracy_score(y_test, voting_clf_pred))\n",
        "    print(classification_report(y_test,voting_clf_pred,target_names=sentiments))\n",
        "\n",
        "    return voting_clf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "a1g91VnGPWAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Confusion matrix & Learning Curve"
      ],
      "metadata": {
        "id": "-Bp-3_mQPhnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def plot_learning_curve_with_auc_and_confusion_matrix(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Fit the estimator\n",
        "    estimator.fit(X, y)\n",
        "\n",
        "    # Plot learning curves\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.subplot(1, 2, 2)\n",
        "    y_pred = estimator.predict(X)\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    cmap = plt.get_cmap('Blues')\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(np.unique(y)))\n",
        "\n",
        "    plt.xticks(tick_marks, np.unique(y), rotation=45)\n",
        "    plt.yticks(tick_marks, np.unique(y))\n",
        "\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "    return plt\n",
        "\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    return plot_learning_curve_with_auc_and_confusion_matrix(estimator, title, X, y, ylim, cv, n_jobs, train_sizes)\n",
        "\n",
        "\n",
        "\n",
        "soft_voting_clf = soft_voting(X1_train, y_train, X1_test, y_test)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A6pR39bcxE1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ROC AUC"
      ],
      "metadata": {
        "id": "HEt1YYnwPm5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc(clf,X_test, y_test):\n",
        "\n",
        "  pred = clf.predict(X_test)\n",
        "  pred_proba = clf.predict_proba(X_test)\n",
        "\n",
        "  fpr = {}\n",
        "  tpr = {}\n",
        "  thresh = {}\n",
        "  roc_auc = {}\n",
        "  n_class = 5\n",
        "  color_mat = ['orange','green','blue','red','purple']\n",
        "\n",
        "  for i in range(n_class):\n",
        "     fpr[i], tpr[i], _ = roc_curve(y_test, pred_proba[:, i], pos_label=i)\n",
        "     roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "     string = 'class' + str(i) + 'vs Rest (AUC = %0.2f)' % roc_auc[i]\n",
        "     plt.plot(fpr[i], tpr[i], linestyle='--', color=color_mat[i], label=string)\n",
        "\n",
        "  plt.title('Multiclass ROC curve')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive rate')\n",
        "  plt.legend(loc='best')\n",
        "  plt.savefig('Multiclass ROC',dpi=300);\n",
        "\n",
        "plot_roc(soft_voting_clf,X1_test,y_test)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rc1faDttzrWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking on ML Flow"
      ],
      "metadata": {
        "id": "KUNRmVGP8LYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "laQHqzBx8U4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ngrok config add-authtoken #my_auth_token"
      ],
      "metadata": {
        "id": "UjhJKSHL8J17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# public_url = ngrok.connect(port = '8050')\n",
        "\n",
        "\n",
        "ngrok.kill()\n",
        "#2RGUGK8CBB79RAtl8U242lWSv2G_a3vux6PfHfVEhjaFDhjV\n",
        "NGROK_AUTH_TOKEN = #My_auth_token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "ngrok_tunnel = ngrok.connect(addr=\"5000\",proto=\"http\",bind_tls=True)\n",
        "print(\"ML Project Tracking UI\",ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "Jx8-8BGw8RHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mlflow ui"
      ],
      "metadata": {
        "id": "MT4gU1ZT8VwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEB APP"
      ],
      "metadata": {
        "id": "u2Ri1zstEWtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing the libraries"
      ],
      "metadata": {
        "id": "y4IyP48VQQ5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow\n",
        "!pip install jupyter-dash\n",
        "!pip install pyngrok\n",
        "!pip install dash\n",
        "!pip install dash_core_components\n",
        "!pip install dash_html_components\n",
        "!pip install emoji\n",
        "!pip install contractions\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "tBJJLEFiQV6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "88qzhqV3QXuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import sys\n",
        "import re\n",
        "import mlflow.sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output\n",
        "from pyngrok import ngrok\n",
        "import unicodedata\n",
        "import contractions\n",
        "import unicodedata\n",
        "import contractions\n",
        "import traceback\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "id": "v4u6btLgQcTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model from MLFLOW"
      ],
      "metadata": {
        "id": "liZaBEAmQrcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MLflow model\n",
        "model_uri = 'runs:/cf57338725f44419a6c5003b2fa19981/softvoting classifier'  # Replace with the URI of your trained MLflow model\n",
        "model = mlflow.sklearn.load_model(model_uri)\n",
        "vectorizer = TfidfVectorizer()\n",
        "#vectorizer.vocabulary_ = vocabulary\n",
        "\n",
        "# Define the classes for cyberbullying detection\n",
        "classes = ['age', 'ethnicity','gender','not_cyberbullying','religion']\n",
        "\n",
        "X_train = df['processed_text']\n",
        "vectorizer.fit(X_train)"
      ],
      "metadata": {
        "id": "jEwZCpszRKf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Enterd Tweet"
      ],
      "metadata": {
        "id": "kKLwlOoRRDAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def emoji_to_text(text):\n",
        "    if type(text) != float:\n",
        "        return emoji.demojize(text)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_underscore_dash(text):\n",
        "  return re.sub(r\"(-|_)\",\" \",text)\n",
        "\n",
        "def remove_hashtags(text):\n",
        "    return text.replace('#','')\n",
        "\n",
        "def remove_mention(text):\n",
        "  return re.sub(r\"@[^\\s]+\",\"\",text)\n",
        "\n",
        "def remove_link(text):\n",
        "  return re.sub(r\"http[^s]+\",\"\",text)\n",
        "\n",
        "def remove_repeated_chars(text):\n",
        "  return re.sub(r\"(.)\\1\\1+\",r\"\\1\",text)\n",
        "\n",
        "def remove_numbers(text):\n",
        "  return re.sub(r'[0-9]','',text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  if not (re.match(r'#(\\w+)', text)):\n",
        "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
        "  return text\n",
        "\n",
        "def remove_non_english_char(text):\n",
        "  return  \"\".join(([idx for idx in text if not re.findall(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\", idx)]))\n",
        "\n",
        "\n",
        "def remove_whitespace(text):\n",
        "\n",
        "    text = re.sub(r'^\\s+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip('\\'')\n",
        "\n",
        "    return text\n",
        "\n",
        "def lower_text(text):\n",
        "  return text.lower()\n",
        "\n",
        "def remove_accents(text):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
        "    text = nfkd_form.encode('ASCII', 'ignore').decode('utf-8')\n",
        "    return text\n",
        "\n",
        "def remove_contractions(text):\n",
        "  expanded_words = []\n",
        "  for word in text.split():\n",
        "      expanded_words.append(contractions.fix(word))\n",
        "\n",
        "  return ' '.join(expanded_words)\n",
        "\n",
        "def remove_duplicated_emojies(text):\n",
        "  return re.sub(r\"([\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF\\U00002600-\\U000027BF])\\1+\",r'\\1',text)\n",
        "\n",
        "def remove_duplicated_words(text):\n",
        "  return re.sub(r\"\\\\b(\\\\w+)(?:\\\\W+\\\\1\\\\b)+\",'',text)\n",
        "\n",
        "\n",
        "def remove_stopwords(text,flag):\n",
        "  if flag:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "  else:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.update(['RT','as','go','rt','would','know','one','u','amp','mkr'],)\n",
        "  tokenizer = TweetTokenizer()\n",
        "  tokenized = tokenizer.tokenize(text)\n",
        "  removed_sentence = [word for word in tokenized if not word.lower() in stop_words]\n",
        "  return ' '.join(removed_sentence)\n",
        "\n",
        "def stemmer(text):\n",
        "   tokenizer = TweetTokenizer()\n",
        "   tokenized = tokenizer.tokenize(text)\n",
        "   stemmer = PorterStemmer()\n",
        "   s = []\n",
        "   for token in tokenized:\n",
        "      token = stemmer.stem(token)\n",
        "      s.append(token)\n",
        "   return ' '.join(s)\n",
        "\n",
        "def lemmatizer(text):\n",
        "  tokenizer = TweetTokenizer()\n",
        "  tokenized = tokenizer.tokenize(text)\n",
        "  lemmatizerr = WordNetLemmatizer()\n",
        "  s = []\n",
        "  for token in tokenized:\n",
        "    token = lemmatizerr.lemmatize(token)\n",
        "    s.append(token)\n",
        "  return ' '.join(s)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(text,flag):\n",
        "\n",
        "  \"\"\"\n",
        "  Process the text and return it to a clean one.\n",
        "  Input:\n",
        "        text: a string containing a text.\n",
        "  Output:\n",
        "        cleaned_text : text after applying all the cleaning functions.\n",
        "  \"\"\"\n",
        "  text = remove_mention(text)\n",
        "  text = remove_link(text)\n",
        "  text = remove_hashtags(text)\n",
        "  text = remove_duplicated_emojies(text)\n",
        "  text = emoji_to_text(text)\n",
        "  text = remove_repeated_chars(text)\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_underscore_dash(text)\n",
        "  text = remove_contractions(text)\n",
        "  text = remove_non_english_char(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = remove_accents(text)\n",
        "  text = lower_text(text)\n",
        "  text = remove_duplicated_words(text)\n",
        "  text = remove_whitespace(text)\n",
        "  text = remove_stopwords(text,flag)\n",
        "  text = lemmatizer(text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "565AFFR9RBFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the web app"
      ],
      "metadata": {
        "id": "SlEfaHHCRNyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create the Dash app\n",
        "app = JupyterDash(__name__)\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"Cyberbullying Detection\"),\n",
        "    dcc.Textarea(\n",
        "        id='input-text',\n",
        "        placeholder=\"Enter the text here...\",\n",
        "        style={'width': '100%', 'height': 100}\n",
        "    ),\n",
        "    html.Button('Submit', id='submit-button', n_clicks=0),\n",
        "    html.Div(id='output-prediction')\n",
        "])\n",
        "\n",
        "# Define the callback function to handle text classification\n",
        "@app.callback(\n",
        "    Output('output-prediction', 'children'),\n",
        "    [Input('submit-button', 'n_clicks')],\n",
        "    [dash.dependencies.State('input-text', 'value')]\n",
        ")\n",
        "\n",
        "\n",
        "def classify_text(n_clicks, text):\n",
        "    if n_clicks > 0 and text:\n",
        "        try:\n",
        "\n",
        "            preprocessed_text = preprocess_text(text, True)\n",
        "            vectorized_text = vectorizer.transform([preprocessed_text]).toarray()\n",
        "\n",
        "            #print(\"Preprocessed Text:\", preprocessed_text)\n",
        "\n",
        "\n",
        "            # Make predictions on the preprocessed text using the trained model\n",
        "            predicted_class = model.predict(vectorized_text)[0]\n",
        "            class_label = classes[predicted_class]\n",
        "\n",
        "            return f\"The text belongs to the class: {class_label}\"\n",
        "        except Exception as e:\n",
        "            traceback_str = traceback.format_exc()\n",
        "            print(traceback_str)\n",
        "            return html.Div([html.Pre(traceback_str, style={'color': 'red'})])\n",
        "            return \"An error occurred during text classification.\"\n",
        "    return ''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run the app and display the result using ngrok\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(mode='external')\n",
        "    ngrok.kill()\n",
        "    public_url = ngrok.connect(addr='localhost:8050')\n",
        "\n",
        "    print(\"Dash app running at:\", public_url)\n",
        "\n"
      ],
      "metadata": {
        "id": "AwbwGX1d-q1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cWVShpTEM85l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R2KJwTPjCMch",
        "4lHuYbMiClQC",
        "q0Fts2KlC0Bo",
        "OmADcucAC8ez",
        "6VxNg-hDDBMc",
        "JFGNkw_JDR01",
        "k3n42NUoIJnw",
        "08BVcl-IIPOv",
        "SRGf-XK0Dpi0",
        "T4cHpyCZD2CU",
        "zl66nFCwOMD4",
        "y-HSckGWE9wH",
        "btUkNXvZFILM",
        "oTB3zjBNFTmI",
        "9S50zWTsFZoN",
        "qdGS4wums4aZ",
        "Nfc5xRaNY4zc",
        "DoVSe7RzMe26",
        "CLgzK2O94g9p",
        "BrRBFfu46Lws",
        "_aY4aiIhZ33Y",
        "i7sbsQyjOg9J",
        "p7G8cpOsNEKW",
        "aQ8KLmEE273z",
        "lRqW6Ppl53JF",
        "OWYluMur6CX6",
        "TYme2TbKO-zE",
        "CprJa63krwiP",
        "a1g91VnGPWAA",
        "-Bp-3_mQPhnw",
        "HEt1YYnwPm5I",
        "D7lmQXwuPxPh",
        "u2Ri1zstEWtO",
        "y4IyP48VQQ5t",
        "88qzhqV3QXuk",
        "liZaBEAmQrcN",
        "kKLwlOoRRDAz",
        "SlEfaHHCRNyb"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}